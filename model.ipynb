{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edb4bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,346\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# install: pip install transformer-lens\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "\n",
    "# Configuration\n",
    "config = HookedTransformerConfig(\n",
    "    n_layers=2,\n",
    "    n_heads=8,\n",
    "    d_model=128,\n",
    "    d_head=16,  # d_model / n_heads\n",
    "    d_mlp=None,  # No MLPs (attention-only)\n",
    "    act_fn=None,  # No activation (no MLPs)\n",
    "    attention_dir=\"causal\",  # Causal attention\n",
    "    attn_only=True,  # Attention-only model\n",
    "    normalization_type=None,  # No LayerNorm for simplicity\n",
    "    d_vocab=50,  # 26 letters + 10 digits + special tokens\n",
    "    n_ctx=50,  # Max sequence length\n",
    "    init_weights=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "model = HookedTransformer(config)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f64205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character-level tokenizer\n",
    "class CountingTokenizer:\n",
    "    def __init__(self):\n",
    "        # Vocabulary: letters + digits + special tokens\n",
    "        self.chars = list(\"abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "        self.special = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \":\", \" \", \"Count\", \"the\", \"letter\", \"in\"]\n",
    "        \n",
    "        self.vocab = self.special + self.chars\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.char_to_id = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.id_to_char = {i: c for i, c in enumerate(self.vocab)}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to token IDs\"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            # Try multi-char tokens first\n",
    "            if text[i:i+5] == \"Count\":\n",
    "                tokens.append(self.char_to_id[\"Count\"])\n",
    "                i += 5\n",
    "            elif text[i:i+3] == \"the\":\n",
    "                tokens.append(self.char_to_id[\"the\"])\n",
    "                i += 3\n",
    "            elif text[i:i+6] == \"letter\":\n",
    "                tokens.append(self.char_to_id[\"letter\"])\n",
    "                i += 6\n",
    "            elif text[i:i+2] == \"in\":\n",
    "                tokens.append(self.char_to_id[\"in\"])\n",
    "                i += 2\n",
    "            else:\n",
    "                tokens.append(self.char_to_id[text[i]])\n",
    "                i += 1\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs to text\"\"\"\n",
    "        return \"\".join([self.id_to_char[i] for i in ids])\n",
    "\n",
    "tokenizer = CountingTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbf56ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Count the letter a in: nidpah1\n",
      "Tokens: [5, 4, 6, 4, 7, 4, 9, 4, 8, 3, 4, 22, 17, 12, 24, 9, 16, 36]\n",
      "Question length: 17\n",
      "Answer: 1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_counting_example(target_letter='a', multiplicity_range=(1, 2), \n",
    "                              length_range=(5, 10), tokenizer=None):\n",
    "    \"\"\"\n",
    "    Generate: \"Count the letter a in: banana\" -> \"3\"\n",
    "    Format: [question tokens] [answer token]\n",
    "    \"\"\"\n",
    "    # Sample words with target letter\n",
    "    words_with_target = []\n",
    "    count = random.randint(*multiplicity_range)\n",
    "    \n",
    "    # Generate string with exact count of target letter\n",
    "    chars = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "    chars.remove(target_letter)\n",
    "    \n",
    "    length = random.randint(*length_range)\n",
    "    string_chars = random.choices(chars, k=length - count)\n",
    "    \n",
    "    # Insert target letters\n",
    "    positions = random.sample(range(length), count)\n",
    "    for pos in positions:\n",
    "        string_chars.insert(pos, target_letter)\n",
    "    \n",
    "    input_string = \"\".join(string_chars[:length])\n",
    "    \n",
    "    # Format question\n",
    "    question = f\"Count the letter {target_letter} in: {input_string}\"\n",
    "    answer = str(count)\n",
    "    \n",
    "    # Tokenize\n",
    "    question_tokens = tokenizer.encode(question)\n",
    "    answer_token = tokenizer.encode(answer)[0]  # Single digit\n",
    "    \n",
    "    # Combine: question + answer\n",
    "    full_tokens = question_tokens + [answer_token]\n",
    "    \n",
    "    return {\n",
    "        'tokens': full_tokens,\n",
    "        'question_length': len(question_tokens),  # For loss masking\n",
    "        'answer': count,\n",
    "        'text': question + answer\n",
    "    }\n",
    "\n",
    "# Test\n",
    "tokenizer = CountingTokenizer()\n",
    "example = generate_counting_example(tokenizer=tokenizer)\n",
    "print(f\"Text: {example['text']}\")\n",
    "print(f\"Tokens: {example['tokens']}\")\n",
    "print(f\"Question length: {example['question_length']}\")\n",
    "print(f\"Answer: {example['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973f38a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 22])\n",
      "Mask shape: torch.Size([64, 22])\n",
      "Example tokens: tensor([ 5,  4,  6,  4,  7,  4, 12,  4,  8,  3,  4, 18, 31, 25, 30, 16, 31, 12,\n",
      "        36,  0,  0,  0])\n",
      "Example mask: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CountingDataset(Dataset):\n",
    "    def __init__(self, n_examples=10000, difficulty='easy', tokenizer=None):\n",
    "        \"\"\"\n",
    "        difficulty: 'easy', 'bpe-hard', 'mult-hard', etc.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        \n",
    "        # Set parameters based on difficulty\n",
    "        if difficulty == 'easy':\n",
    "            mult_range = (1, 2)\n",
    "            len_range = (5, 10)\n",
    "        elif difficulty == 'mult-hard':\n",
    "            mult_range = (3, 10)\n",
    "            len_range = (5, 10)\n",
    "        elif difficulty == 'length-hard':\n",
    "            mult_range = (1, 2)\n",
    "            len_range = (20, 50)\n",
    "        elif difficulty == 'all-hard':\n",
    "            mult_range = (3, 10)\n",
    "            len_range = (20, 50)\n",
    "        \n",
    "        # Generate examples\n",
    "        target_letters = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "        for _ in range(n_examples):\n",
    "            target = random.choice(target_letters)\n",
    "            example = generate_counting_example(\n",
    "                target_letter=target,\n",
    "                multiplicity_range=mult_range,\n",
    "                length_range=len_range,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "            self.examples.append(example)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def collate_fn(batch, pad_id=0, max_len=50):\n",
    "    \"\"\"Pad sequences to same length\"\"\"\n",
    "    # Pad tokens\n",
    "    tokens = [ex['tokens'] for ex in batch]\n",
    "    max_batch_len = min(max(len(t) for t in tokens), max_len)\n",
    "    \n",
    "    padded_tokens = []\n",
    "    masks = []  # Loss mask: 1 for answer token, 0 elsewhere\n",
    "    \n",
    "    for ex in batch:\n",
    "        seq = ex['tokens'][:max_batch_len]\n",
    "        q_len = min(ex['question_length'], max_batch_len - 1)\n",
    "        \n",
    "        # Pad sequence\n",
    "        padded = seq + [pad_id] * (max_batch_len - len(seq))\n",
    "        padded_tokens.append(padded)\n",
    "        \n",
    "        # Create mask: only compute loss on answer token\n",
    "        mask = [0] * max_batch_len\n",
    "        if q_len < len(seq):  # If answer token exists\n",
    "            mask[q_len] = 1  # Answer is right after question\n",
    "        masks.append(mask)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(padded_tokens, dtype=torch.long),\n",
    "        'loss_mask': torch.tensor(masks, dtype=torch.float),\n",
    "        'answers': torch.tensor([ex['answer'] for ex in batch], dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# Create dataloaders\n",
    "tokenizer = CountingTokenizer()\n",
    "train_dataset = CountingDataset(n_examples=10000, difficulty='easy', tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, \n",
    "                         collate_fn=collate_fn)\n",
    "\n",
    "# Test batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Input shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Mask shape: {batch['loss_mask'].shape}\")\n",
    "print(f\"Example tokens: {batch['input_ids'][0]}\")\n",
    "print(f\"Example mask: {batch['loss_mask'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa2db8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 157/157 [00:01<00:00, 102.28it/s, loss=0.691, acc=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.8805, Acc=0.4994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 157/157 [00:01<00:00, 126.94it/s, loss=0.775, acc=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.6979, Acc=0.4988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 157/157 [00:01<00:00, 127.86it/s, loss=0.739, acc=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.6984, Acc=0.5085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 157/157 [00:01<00:00, 126.14it/s, loss=0.699, acc=0.506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.7002, Acc=0.5064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 157/157 [00:01<00:00, 130.63it/s, loss=0.678, acc=0.505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.6970, Acc=0.5054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 157/157 [00:01<00:00, 126.98it/s, loss=0.674, acc=0.516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.6925, Acc=0.5162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 157/157 [00:01<00:00, 131.47it/s, loss=0.606, acc=0.536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=0.6901, Acc=0.5359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 157/157 [00:01<00:00, 128.25it/s, loss=0.791, acc=0.571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=0.6801, Acc=0.5705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 157/157 [00:01<00:00, 131.59it/s, loss=0.627, acc=0.588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=0.6718, Acc=0.5879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 157/157 [00:01<00:00, 128.31it/s, loss=0.694, acc=0.609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.6591, Acc=0.6091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 157/157 [00:01<00:00, 121.91it/s, loss=0.722, acc=0.619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=0.6502, Acc=0.6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 157/157 [00:01<00:00, 127.83it/s, loss=0.678, acc=0.64] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=0.6353, Acc=0.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 157/157 [00:01<00:00, 131.44it/s, loss=0.616, acc=0.665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.6075, Acc=0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 157/157 [00:01<00:00, 128.01it/s, loss=0.543, acc=0.703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=0.5659, Acc=0.7031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 157/157 [00:01<00:00, 131.10it/s, loss=0.632, acc=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=0.4915, Acc=0.7613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 157/157 [00:01<00:00, 121.47it/s, loss=0.374, acc=0.83] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss=0.3896, Acc=0.8304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 157/157 [00:01<00:00, 128.89it/s, loss=0.199, acc=0.883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss=0.2957, Acc=0.8828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 157/157 [00:01<00:00, 130.23it/s, loss=0.44, acc=0.912] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss=0.2418, Acc=0.9118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 157/157 [00:01<00:00, 129.62it/s, loss=0.13, acc=0.93]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss=0.2094, Acc=0.9302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 157/157 [00:01<00:00, 129.49it/s, loss=0.131, acc=0.937]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss=0.1973, Acc=0.9371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, n_epochs=10, lr=1e-3, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train with MASKED loss (only on answer token)\n",
    "    \n",
    "    Classification loss: CrossEntropy on vocabulary\n",
    "    (Can also try regression loss on digit value)\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "    \n",
    "    # Classification loss (predict token ID)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')  # Per-token loss\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)  # [batch, seq_len]\n",
    "            loss_mask = batch['loss_mask'].to(device)  # [batch, seq_len]\n",
    "            \n",
    "            # Forward pass\n",
    "            # Input: all tokens except last\n",
    "            # Target: all tokens except first (shifted by 1)\n",
    "            logits = model(input_ids[:, :-1])  # [batch, seq_len-1, vocab]\n",
    "            targets = input_ids[:, 1:]  # [batch, seq_len-1]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_per_token = criterion(\n",
    "                logits.reshape(-1, logits.size(-1)),  # [batch*(seq_len-1), vocab]\n",
    "                targets.reshape(-1)  # [batch*(seq_len-1)]\n",
    "            )\n",
    "            loss_per_token = loss_per_token.reshape(targets.shape)  # [batch, seq_len-1]\n",
    "            \n",
    "            # Apply mask: only compute loss on answer token\n",
    "            mask = loss_mask[:, 1:]  # Align with targets\n",
    "            masked_loss = (loss_per_token * mask).sum() / mask.sum()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            masked_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += masked_loss.item()\n",
    "            \n",
    "            # Accuracy: check if predicted answer digit is correct\n",
    "            preds = logits.argmax(dim=-1)  # [batch, seq_len-1]\n",
    "            answer_positions = mask.bool()\n",
    "            if answer_positions.any():\n",
    "                correct += (preds[answer_positions] == targets[answer_positions]).sum().item()\n",
    "                total += answer_positions.sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': masked_loss.item(), \n",
    "                            'acc': correct/total if total > 0 else 0})\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Acc={correct/total:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train!\n",
    "model = HookedTransformer(config)\n",
    "trained_model = train_model(model, train_loader, n_epochs=20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ef2db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Count the letter a in: banana\n",
      "Predicted: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model(model, text, tokenizer, device='cuda'):\n",
    "    \"\"\"Quick inference test\"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)\n",
    "        pred_token = logits[0, -1].argmax().item()\n",
    "        pred_char = tokenizer.id_to_char[pred_token]\n",
    "    \n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Predicted: {pred_char}\")\n",
    "    return pred_char\n",
    "\n",
    "# Test\n",
    "test_model(trained_model, \"Count the letter a in: banana\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2edc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
