{
 "cells": [
  {
   "cell_type": "raw",
   "id": "73cb6f89-0245-42ae-9f4d-be62cadc2d83",
   "metadata": {},
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edb4bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 152,626\n",
      "Model device: cuda:0\n",
      "Model device name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "\n",
    "# Configuration\n",
    "config = HookedTransformerConfig(\n",
    "    n_layers=2,\n",
    "    n_heads=8,\n",
    "    d_model=128,\n",
    "    d_head=16,  # d_model / n_heads\n",
    "    d_mlp=None,  # No MLPs (attention-only)\n",
    "    act_fn=None,  # No activation (no MLPs)\n",
    "    attention_dir=\"causal\",  # Causal attention\n",
    "    attn_only=True,  # Attention-only model\n",
    "    normalization_type=None,  # No LayerNorm for simplicity\n",
    "    d_vocab=50,  # 26 letters + 10 digits + special tokens\n",
    "    n_ctx=60,  # Max sequence length\n",
    "    init_weights=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "model = HookedTransformer(config)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d59195-2df3-4d0c-bf1d-bce5105aa3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    class TrieNode:\n",
    "        def __init__(self):\n",
    "            self.id = None\n",
    "            self.next = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = self.TrieNode()\n",
    "        self.token_map = {} # Stores mapping of ID to token\n",
    "        self.size = 0\n",
    "\n",
    "    # Adds a new token into the vocabulary\n",
    "    def add_token(self, token):\n",
    "        node = self.root\n",
    "        for c in token:\n",
    "            if c not in node.next:\n",
    "                node.next[c] = self.TrieNode()\n",
    "            node = node.next[c]\n",
    "        if node.id is None:\n",
    "            node.id = self.size\n",
    "            self.token_map[self.size] = token\n",
    "            self.size += 1\n",
    "\n",
    "    # Finds id of longest prefix token of text[start:end], and returns length of token\n",
    "    def longest_prefix_token(self, text, start):\n",
    "        longest_token = None\n",
    "        longest_length = 0\n",
    "        node = self.root\n",
    "        for i in range(start, len(text)):\n",
    "            if text[i] not in node.next:\n",
    "                break\n",
    "            node = node.next[text[i]]\n",
    "            if node.id is not None:\n",
    "                longest_token = node.id\n",
    "                longest_length = i - start + 1\n",
    "        assert longest_token is not None\n",
    "        return longest_token, longest_length\n",
    "\n",
    "    # Converts an id to the corresponding token\n",
    "    def get_token(self, id):\n",
    "        return self.token_map[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f64205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple character-level tokenizer\n",
    "class CountingTokenizer:\n",
    "    def __init__(self):\n",
    "        # Vocabulary: letters + digits + special tokens\n",
    "        self.vocab = Vocabulary()\n",
    "        chars = list(\"abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "        special = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \":\", \" \", \"Count\", \"the\", \"letter\", \"in\"]\n",
    "        raw_vocab = special + chars\n",
    "        for token in raw_vocab:\n",
    "            self.vocab.add_token(token)\n",
    "\n",
    "    def encode(self, text, include_lengths = False):\n",
    "        \"\"\"Convert text to token IDs\"\"\"\n",
    "        ids = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            id, token_length = self.vocab.longest_prefix_token(text, i)\n",
    "            assert id != -1\n",
    "            if include_lengths:\n",
    "                ids.append((id, token_length))\n",
    "            else:\n",
    "                ids.append(id)\n",
    "            i += token_length\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs to text\"\"\"\n",
    "        return \"\".join([self.vocab.get_token(id) for id in ids])\n",
    "\n",
    "    def apply_bpe(self, words, max_token_length=3):\n",
    "        \"\"\"Adds merge rules based on a list of words for BPE\"\"\"\n",
    "        text = \"\".join([f\"<BOS>{word}<EOS>\" for word in words])\n",
    "        ignore_tokens = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \":\", \" \"]\n",
    "        while True:\n",
    "            encoded = self.encode(text, include_lengths=True)\n",
    "            pairs = {}\n",
    "            merge_pair = ()\n",
    "            for i in range(len(encoded) - 1):\n",
    "                token_pair = encoded[i], encoded[i + 1]\n",
    "                if token_pair[0][1] + token_pair[1][1] > max_token_length:\n",
    "                    continue\n",
    "                if any(self.vocab.get_token(token[0]) in ignore_tokens for token in token_pair):\n",
    "                    continue\n",
    "                pairs[token_pair] = pairs.get(token_pair, 0) + 1\n",
    "                if not merge_pair or pairs[merge_pair] < pairs[token_pair]:\n",
    "                    merge_pair = token_pair\n",
    "            if not merge_pair or pairs[merge_pair] < 2:\n",
    "                break\n",
    "            self.vocab.add_token(\"\".join([self.vocab.get_token(token[0]) for token in merge_pair]))\n",
    "\n",
    "tokenizer = CountingTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbf56ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Count the letter a in: avekjaj2\n",
      "Tokens: [5, 4, 6, 4, 7, 4, 9, 4, 8, 3, 4, 9, 30, 13, 19, 18, 9, 18, 37]\n",
      "Question Tokens Decoded: ['Count', ' ', 'the', ' ', 'letter', ' ', 'a', ' ', 'in', ':', ' ', 'a', 'v', 'e', 'k', 'j', 'a', 'j']\n",
      "Question length: 18\n",
      "Answer: 2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_counting_question(target_letter='a', multiplicity_range=(1, 2), length_range=(5, 10)):\n",
    "    \"\"\"\n",
    "    Generates a new word based on input parameters and gives the answer to the question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample words with target letter\n",
    "    words_with_target = []\n",
    "    count = random.randint(*multiplicity_range)\n",
    "    \n",
    "    # Generate string with exact count of target letter\n",
    "    chars = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "    chars.remove(target_letter)\n",
    "    \n",
    "    length = random.randint(max(count, length_range[0]), length_range[1])\n",
    "    string_chars = random.choices(chars, k=length - count)\n",
    "    \n",
    "    # Insert target letters\n",
    "    positions = random.sample(range(length), count)\n",
    "    for pos in positions:\n",
    "        string_chars.insert(pos, target_letter)\n",
    "    \n",
    "    input_string = \"\".join(string_chars[:length])\n",
    "    question = f\"Count the letter {target_letter} in: {input_string}\"\n",
    "    answer = str(count)\n",
    "\n",
    "    return question, answer\n",
    "\n",
    "def generate_counting_example(qa, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Generate: \"Count the letter a in: banana\" -> \"3\"\n",
    "    Format: [question tokens] [answer token]\n",
    "    \"\"\"\n",
    "    question, answer = qa\n",
    "    \n",
    "    # Tokenize\n",
    "    question_tokens = tokenizer.encode(question)\n",
    "    question_tokens_decoded = [tokenizer.decode([token]) for token in question_tokens]\n",
    "    answer_token = tokenizer.encode(answer)[0]  # Single digit\n",
    "    \n",
    "    # Combine: question + answer\n",
    "    full_tokens = question_tokens + [answer_token]\n",
    "    \n",
    "    return {\n",
    "        'tokens': full_tokens,\n",
    "        'question_tokens_decoded': question_tokens_decoded,\n",
    "        'question_length': len(question_tokens),  # For loss masking\n",
    "        'answer': int(answer),\n",
    "        'text': question + answer\n",
    "    }\n",
    "\n",
    "# Test\n",
    "tokenizer = CountingTokenizer()\n",
    "qa = generate_counting_question()\n",
    "example = generate_counting_example(qa, tokenizer=tokenizer)\n",
    "print(f\"Text: {example['text']}\")\n",
    "print(f\"Tokens: {example['tokens']}\")\n",
    "print(f\"Question Tokens Decoded: {example['question_tokens_decoded']}\")\n",
    "print(f\"Question length: {example['question_length']}\")\n",
    "print(f\"Answer: {example['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "096c9875-c66d-48e8-8b77-a82023f79280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Pre-Processing Text: <BOS>Count the letter a in: dvdabc<EOS><BOS>Count the letter a in: afzceac<EOS><BOS>Count the letter a in: yonchwdash<EOS><BOS>Count the letter a in: papmabm<EOS><BOS>Count the letter a in: yoqgam<EOS><BOS>Count the letter a in: ajqna<EOS><BOS>Count the letter a in: kagvhda<EOS><BOS>Count the letter a in: hztavv<EOS><BOS>Count the letter a in: afxae<EOS><BOS>Count the letter a in: auppacfb<EOS>\n",
      "Text: Count the letter a in: fakfwih1\n",
      "Tokens: [5, 4, 6, 4, 7, 4, 9, 4, 8, 3, 4, 14, 9, 19, 14, 31, 17, 16, 36]\n",
      "Question Tokens Decoded: ['Count', ' ', 'the', ' ', 'letter', ' ', 'a', ' ', 'in', ':', ' ', 'f', 'a', 'k', 'f', 'w', 'i', 'h']\n",
      "Question length: 18\n",
      "Answer: 1\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "qas = [generate_counting_question() for i in range(10)]\n",
    "tokenizer = CountingTokenizer()\n",
    "tokenizer.apply_bpe([qa[0] for qa in qas])\n",
    "qa = generate_counting_question()\n",
    "example = generate_counting_example(qa, tokenizer=tokenizer)\n",
    "print(f\"Sample Pre-Processing Text: {\"\".join([f\"<BOS>{qa[0]}<EOS>\" for qa in qas])}\")\n",
    "print(f\"Text: {example['text']}\")\n",
    "print(f\"Tokens: {example['tokens']}\")\n",
    "print(f\"Question Tokens Decoded: {example['question_tokens_decoded']}\")\n",
    "print(f\"Question length: {example['question_length']}\")\n",
    "print(f\"Answer: {example['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0f8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CountingDataset(Dataset):\n",
    "    def __init__(self, n_examples=50000, difficulty='easy', tokenizer=None, allow_bpe=True, train_bpe=True):\n",
    "        \"\"\"\n",
    "        difficulty: 'easy', 'bpe-hard', 'mult-hard', etc.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "\n",
    "        use_bpe = False\n",
    "        # Set parameters based on difficulty\n",
    "        if difficulty == 'easy':\n",
    "            mult_range = (1, 2)\n",
    "            len_range = (5, 10)\n",
    "        elif difficulty == 'bpe-hard':\n",
    "            mult_range = (1, 2)\n",
    "            len_range = (5, 10)\n",
    "            # use_bpe = True\n",
    "        elif difficulty == 'mult-hard':\n",
    "            mult_range = (3, 10)\n",
    "            len_range = (5, 10)\n",
    "        elif difficulty == 'length-hard':\n",
    "            mult_range = (1, 2)\n",
    "            len_range = (20, 50)\n",
    "        elif difficulty == 'all-hard':\n",
    "            mult_range = (3, 10)\n",
    "            len_range = (20, 50)\n",
    "            # use_bpe = True\n",
    "        elif difficulty == 'mixed':\n",
    "            mult_range = (1,10)\n",
    "            len_range = (5, 50)\n",
    "            # use_bpe = True\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "        # Generate basic words\n",
    "        target_letters = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "        qas = []\n",
    "        for _ in range(n_examples):\n",
    "            target = random.choice(target_letters)\n",
    "            qa = generate_counting_question(\n",
    "                target_letter=target,\n",
    "                multiplicity_range=mult_range,\n",
    "                length_range=len_range,\n",
    "            )\n",
    "            qas.append(qa)\n",
    "\n",
    "        # Add basic words to the vocabulary if we are applying BPE\n",
    "        if use_bpe and allow_bpe and train_bpe:\n",
    "            tokenizer.apply_bpe([qa[0] for qa in qas])\n",
    "\n",
    "        # Generate the full questions and examples\n",
    "        basic_tokenizer = CountingTokenizer()\n",
    "        if difficulty == \"mixed\":\n",
    "            bpe_set = random.sample(range(len(qas)), len(qas) // 2)\n",
    "        else:\n",
    "            bpe_set = range(len(qas))\n",
    "        for i in range(len(qas)):\n",
    "            example = generate_counting_example(\n",
    "                qas[i],\n",
    "                tokenizer=tokenizer if i in bpe_set else basic_tokenizer\n",
    "            )\n",
    "            self.examples.append(example)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def collate_fn(batch, pad_id=0, max_len=100):\n",
    "    \"\"\"Pad sequences to same length\"\"\"\n",
    "    # Pad tokens\n",
    "    tokens = [ex['tokens'] for ex in batch]\n",
    "    max_batch_len = min(max(len(t) for t in tokens), max_len)\n",
    "    \n",
    "    padded_tokens = []\n",
    "    masks = []  # Loss mask: 1 for answer token, 0 elsewhere\n",
    "    \n",
    "    for ex in batch:\n",
    "        seq = ex['tokens'][:max_batch_len]\n",
    "        q_len = min(ex['question_length'], max_batch_len - 1)\n",
    "        \n",
    "        # Pad sequence\n",
    "        padded = seq + [pad_id] * (max_batch_len - len(seq))\n",
    "        padded_tokens.append(padded)\n",
    "        \n",
    "        # Create mask: only compute loss on answer token\n",
    "        mask = [0] * max_batch_len\n",
    "        if q_len < len(seq):  # If answer token exists\n",
    "            mask[q_len] = 1  # Answer is right after question\n",
    "        masks.append(mask)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(padded_tokens, dtype=torch.long),\n",
    "        'loss_mask': torch.tensor(masks, dtype=torch.float),\n",
    "        'answers': torch.tensor([ex['answer'] for ex in batch], dtype=torch.long)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "973f38a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 22])\n",
      "Mask shape: torch.Size([64, 22])\n",
      "Example tokens: tensor([ 5,  4,  6,  4,  7,  4, 15,  4,  8,  3,  4, 34, 33, 30, 15, 15, 23, 14,\n",
      "        37,  0,  0,  0])\n",
      "Example mask: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "train_dataset_names = [\"easy\", \"bpe-hard\", \"mult-hard\", \"length-hard\", \"all-hard\"]\n",
    "GENERATE_NEW_TRAINING_DATA = True # IMPORTANT: Set this to True only if you want to generate new datasets\n",
    "\n",
    "# Training dataloaders\n",
    "train_datasets = {}\n",
    "train_loaders = {}\n",
    "train_tokenizers = {}\n",
    "for name in train_dataset_names:\n",
    "    if GENERATE_NEW_TRAINING_DATA:\n",
    "        train_tokenizers[name] = CountingTokenizer()\n",
    "        train_datasets[name] = CountingDataset(n_examples=100000, difficulty=name, tokenizer=train_tokenizers[name])\n",
    "        with open(f\"train-{name}-dataset.pkl\", \"wb\") as f:\n",
    "            pickle.dump(train_datasets[name], f)\n",
    "        with open(f\"train-{name}-tokenizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(train_tokenizers[name], f)\n",
    "    else:\n",
    "        with open(f\"train-{name}-dataset.pkl\", \"rb\") as f:\n",
    "            train_datasets[name] = pickle.load(f)\n",
    "        with open(f\"train-{name}-tokenizer.pkl\", \"rb\") as f:\n",
    "            train_tokenizers[name] = pickle.load(f)\n",
    "            print(train_tokenizers[name].vocab.size)\n",
    "    train_loaders[name] = DataLoader(train_datasets[name], batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Test batch\n",
    "batch = next(iter(train_loaders[\"bpe-hard\"]))\n",
    "print(f\"Input shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Mask shape: {batch['loss_mask'].shape}\")\n",
    "print(f\"Example tokens: {batch['input_ids'][0]}\")\n",
    "print(f\"Example mask: {batch['loss_mask'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5241ff67-7c6a-4c23-b58f-2f1ca79e73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_NEW_TESTING_DATA = True # IMPORTANT: Set this to True only if you want to generate new datasets\n",
    "\n",
    "# Testing dataloaders\n",
    "test_dataset_names = [\"easy\", \"bpe-hard\", \"mult-hard\", \"length-hard\", \"all-hard\"]\n",
    "test_datasets = {}\n",
    "test_tokenizers = {}\n",
    "for name in test_dataset_names:\n",
    "    if GENERATE_NEW_TESTING_DATA:\n",
    "        test_datasets[name] = CountingDataset(n_examples=10000, difficulty=name, tokenizer=train_tokenizers[name], allow_bpe=True, train_bpe=False)\n",
    "        with open(f\"test-{name}-dataset.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_datasets[name], f)\n",
    "    else:\n",
    "        with open(f\"test-{name}-dataset.pkl\", \"rb\") as f:\n",
    "            test_datasets[name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2db8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train model for easy with vocab size 45\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|███████████████████████████████| 1563/1563 [00:05<00:00, 296.39it/s, loss=0.669, acc=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.7359, Acc=0.4995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|████████████████████████████████| 1563/1563 [00:04<00:00, 314.92it/s, loss=0.71, acc=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.6961, Acc=0.5013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|█████████████████████████████| 1563/1563 [00:04<00:00, 321.24it/s, loss=0.00534, acc=0.678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.5064, Acc=0.6780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|████████████████████████████| 1563/1563 [00:04<00:00, 318.27it/s, loss=0.000526, acc=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.0193, Acc=0.9959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|█████████████████████████████| 1563/1563 [00:04<00:00, 317.42it/s, loss=0.00139, acc=0.997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.0138, Acc=0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|█████████████████████████████| 1563/1563 [00:04<00:00, 319.07it/s, loss=0.00138, acc=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.0092, Acc=0.9981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|████████████████████████████| 1563/1563 [00:04<00:00, 319.15it/s, loss=0.000288, acc=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=0.0074, Acc=0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|█████████████████████████████| 1563/1563 [00:05<00:00, 307.81it/s, loss=7.86e-5, acc=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=0.0050, Acc=0.9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|████████████████████████████| 1563/1563 [00:05<00:00, 309.81it/s, loss=0.000267, acc=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=0.0029, Acc=0.9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 311.63it/s, loss=0.00318, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.0020, Acc=0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 309.93it/s, loss=1.35e-5, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=0.0016, Acc=0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 308.79it/s, loss=4.12e-6, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=0.0007, Acc=0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 303.48it/s, loss=7.66e-6, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.0008, Acc=0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 305.15it/s, loss=2.13e-6, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=0.0004, Acc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|███████████████████████████████| 1563/1563 [00:05<00:00, 307.56it/s, loss=0.000211, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=0.0002, Acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 306.52it/s, loss=4.48e-6, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss=0.0002, Acc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 311.39it/s, loss=8.07e-5, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss=0.0001, Acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 308.89it/s, loss=1.27e-6, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss=0.0000, Acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 312.37it/s, loss=6.46e-6, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss=0.0000, Acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 312.28it/s, loss=1.26e-6, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss=0.0000, Acc=1.0000\n",
      "Starting to train model for bpe-hard with vocab size 45\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|█████████████████████████████████| 1563/1563 [00:05<00:00, 312.50it/s, loss=0.703, acc=0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.7356, Acc=0.5001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|███████████████████████████████| 1563/1563 [00:05<00:00, 312.21it/s, loss=0.692, acc=0.503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.6958, Acc=0.5031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|███████████████████████████████| 1563/1563 [00:05<00:00, 309.68it/s, loss=0.0565, acc=0.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.5963, Acc=0.6402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████████████████████████| 1563/1563 [00:05<00:00, 310.34it/s, loss=0.0545, acc=0.984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.0497, Acc=0.9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|█████████████████████████████| 1563/1563 [00:05<00:00, 312.34it/s, loss=0.00399, acc=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.0190, Acc=0.9946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|████████████████████████████| 1563/1563 [00:04<00:00, 318.66it/s, loss=0.000658, acc=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.0146, Acc=0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████████████████████████| 1563/1563 [00:04<00:00, 320.56it/s, loss=0.0747, acc=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=0.0096, Acc=0.9976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|█████████████████████████████| 1563/1563 [00:04<00:00, 320.21it/s, loss=0.00143, acc=0.997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=0.0106, Acc=0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|█████████████████████████████| 1563/1563 [00:04<00:00, 318.82it/s, loss=0.00454, acc=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=0.0071, Acc=0.9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|███████████████████████████| 1563/1563 [00:04<00:00, 321.00it/s, loss=0.000826, acc=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.0058, Acc=0.9986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████████████████████████| 1563/1563 [00:04<00:00, 324.50it/s, loss=0.126, acc=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=0.0040, Acc=0.9992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|████████████████████████████| 1563/1563 [00:04<00:00, 320.65it/s, loss=0.00226, acc=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=0.0030, Acc=0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|████████████████████████████| 1563/1563 [00:04<00:00, 321.84it/s, loss=2.02e-5, acc=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.0031, Acc=0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|████████████████████████████████| 1563/1563 [00:04<00:00, 329.22it/s, loss=5.01e-5, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=0.0019, Acc=0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|████████████████████████████████| 1563/1563 [00:04<00:00, 318.19it/s, loss=3.21e-5, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=0.0013, Acc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|███████████████████████████████████| 1563/1563 [00:04<00:00, 320.58it/s, loss=2e-5, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss=0.0010, Acc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|████████████████████████████████| 1563/1563 [00:04<00:00, 323.89it/s, loss=2.19e-5, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss=0.0006, Acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|████████████████████████████████| 1563/1563 [00:04<00:00, 323.31it/s, loss=4.39e-5, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss=0.0006, Acc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|████████████████████████████████| 1563/1563 [00:04<00:00, 322.33it/s, loss=1.73e-5, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss=0.0004, Acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|████████████████████████████████| 1563/1563 [00:04<00:00, 324.82it/s, loss=1.41e-5, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss=0.0004, Acc=1.0000\n",
      "Starting to train model for mult-hard with vocab size 45\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|███████████████████████████████| 1563/1563 [00:04<00:00, 321.52it/s, loss=0.954, acc=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=1.7476, Acc=0.2985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|███████████████████████████████| 1563/1563 [00:04<00:00, 326.90it/s, loss=0.802, acc=0.639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.8203, Acc=0.6391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|███████████████████████████████| 1563/1563 [00:04<00:00, 317.98it/s, loss=0.507, acc=0.767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.5470, Acc=0.7672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|███████████████████████████████| 1563/1563 [00:04<00:00, 322.39it/s, loss=0.109, acc=0.861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.3499, Acc=0.8609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|███████████████████████████████| 1563/1563 [00:04<00:00, 316.77it/s, loss=0.151, acc=0.905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.2475, Acc=0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████████████████████████| 1563/1563 [00:04<00:00, 321.48it/s, loss=0.0837, acc=0.936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.1766, Acc=0.9359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|███████████████████████████████| 1563/1563 [00:04<00:00, 317.38it/s, loss=0.103, acc=0.954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=0.1289, Acc=0.9537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████████████████████████| 1563/1563 [00:04<00:00, 322.24it/s, loss=0.0288, acc=0.971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=0.0858, Acc=0.9710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|███████████████████████████████| 1563/1563 [00:04<00:00, 324.30it/s, loss=0.0214, acc=0.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=0.0648, Acc=0.9799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|█████████████████████████████| 1563/1563 [00:04<00:00, 336.23it/s, loss=0.0102, acc=0.987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.0415, Acc=0.9873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████████████████████████| 1563/1563 [00:04<00:00, 328.49it/s, loss=0.0155, acc=0.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=0.0319, Acc=0.9903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|███████████████████████████| 1563/1563 [00:04<00:00, 326.87it/s, loss=0.000745, acc=0.993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=0.0243, Acc=0.9926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|█████████████████████████████| 1563/1563 [00:04<00:00, 322.39it/s, loss=0.0352, acc=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.0176, Acc=0.9949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|█████████████████████████████| 1563/1563 [00:04<00:00, 330.85it/s, loss=0.0015, acc=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=0.0128, Acc=0.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|█████████████████████████████| 1563/1563 [00:04<00:00, 326.14it/s, loss=0.0152, acc=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=0.0088, Acc=0.9977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|█████████████████████████████| 1563/1563 [00:04<00:00, 321.62it/s, loss=0.0323, acc=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss=0.0066, Acc=0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|████████████████████████████| 1563/1563 [00:04<00:00, 324.04it/s, loss=0.00534, acc=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss=0.0049, Acc=0.9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|████████████████████████████| 1563/1563 [00:04<00:00, 315.60it/s, loss=0.00322, acc=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss=0.0039, Acc=0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|███████████████████████████| 1563/1563 [00:05<00:00, 306.18it/s, loss=0.000173, acc=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss=0.0031, Acc=0.9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|████████████████████████████████| 1563/1563 [00:05<00:00, 305.27it/s, loss=0.00209, acc=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss=0.0029, Acc=0.9996\n",
      "Starting to train model for length-hard with vocab size 45\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|███████████████████████████████| 1563/1563 [00:05<00:00, 276.59it/s, loss=0.693, acc=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.7399, Acc=0.4987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████████████████████████████| 1563/1563 [00:05<00:00, 283.11it/s, loss=0.68, acc=0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.6964, Acc=0.5001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|███████████████████████████████| 1563/1563 [00:05<00:00, 276.75it/s, loss=0.696, acc=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.6950, Acc=0.5006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:  89%|███████████████████████████▍   | 1384/1563 [00:04<00:00, 283.67it/s, loss=0.693, acc=0.503]"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "RETRAIN = True # IMPORTANT: Only toggle to True if you want to actually train new models\n",
    "\n",
    "def train_model(model, name, train_loader, n_epochs=10, lr=1e-3, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train with MASKED loss (only on answer token)\n",
    "    \n",
    "    Classification loss: CrossEntropy on vocabulary\n",
    "    (Can also try regression loss on digit value)\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "    \n",
    "    # Classification loss (predict token ID)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')  # Per-token loss\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)  # [batch, seq_len]\n",
    "            loss_mask = batch['loss_mask'].to(device)  # [batch, seq_len]\n",
    "            \n",
    "            # Forward pass\n",
    "            # Input: all tokens except last\n",
    "            # Target: all tokens except first (shifted by 1)\n",
    "            logits = model(input_ids[:, :-1])  # [batch, seq_len-1, vocab]\n",
    "            targets = input_ids[:, 1:]  # [batch, seq_len-1]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_per_token = criterion(\n",
    "                logits.reshape(-1, logits.size(-1)),  # [batch*(seq_len-1), vocab]\n",
    "                targets.reshape(-1)  # [batch*(seq_len-1)]\n",
    "            )\n",
    "            loss_per_token = loss_per_token.reshape(targets.shape)  # [batch, seq_len-1]\n",
    "            \n",
    "            # Apply mask: only compute loss on answer token\n",
    "            mask = loss_mask[:, 1:]  # Align with targets\n",
    "            masked_loss = (loss_per_token * mask).sum() / mask.sum()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            masked_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += masked_loss.item()\n",
    "            \n",
    "            # Accuracy: check if predicted answer digit is correct\n",
    "            preds = logits.argmax(dim=-1)  # [batch, seq_len-1]\n",
    "            answer_positions = mask.bool()\n",
    "            if answer_positions.any():\n",
    "                correct += (preds[answer_positions] == targets[answer_positions]).sum().item()\n",
    "                total += answer_positions.sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': masked_loss.item(), \n",
    "                            'acc': correct/total if total > 0 else 0})\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Acc={correct/total:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'checkpoint-{name}-epoch-{epoch+1}.pt')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train!\n",
    "models = {}\n",
    "for name in train_dataset_names:\n",
    "    config = HookedTransformerConfig(\n",
    "            n_layers=2,\n",
    "            n_heads=8,\n",
    "            d_model=256,\n",
    "            d_head=16,  # d_model / n_heads\n",
    "            d_mlp=None,  # No MLPs (attention-only)\n",
    "            act_fn=None,  # No activation (no MLPs)\n",
    "            attention_dir=\"causal\",  # Causal attention\n",
    "            attn_only=True,  # Attention-only model\n",
    "            normalization_type=None,  # No LayerNorm for simplicity\n",
    "            d_vocab=train_tokenizers[name].vocab.size + 5,  # Total vocab size for particular tokenizer\n",
    "            n_ctx=100,  # Max sequence length\n",
    "            init_weights=True,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "    model = HookedTransformer(config)\n",
    "    if RETRAIN:\n",
    "        print(f\"Starting to train model for {name} with vocab size {train_tokenizers[name].vocab.size}\")\n",
    "        models[name] = train_model(model, name, train_loaders[name], n_epochs=20, lr=3e-4)\n",
    "    else:\n",
    "        checkpoint = torch.load(f'checkpoint-{name}-epoch-40.pt', map_location=\"cpu\")\n",
    "        \n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        models[name] = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            loss_mask = batch[\"loss_mask\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids[:, :-1])\n",
    "            targets = input_ids[:, 1:]\n",
    "            mask = loss_mask[:, 1:]\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            \n",
    "            # Identify valid rows\n",
    "            per_row = mask.sum(dim=1)\n",
    "            valid_rows = (per_row == 1)\n",
    "            \n",
    "            if not valid_rows.any():\n",
    "                continue   # skip batches with no valid rows\n",
    "            \n",
    "            # Filter only valid rows\n",
    "            mask_valid = mask[valid_rows].bool()\n",
    "            preds_valid = preds[valid_rows]\n",
    "            targets_valid = targets[valid_rows]\n",
    "            \n",
    "            # Extract predictions and true labels at masked positions\n",
    "            pred_answers = preds_valid[mask_valid]\n",
    "            true_answers = targets_valid[mask_valid]\n",
    "            \n",
    "            correct += (pred_answers == true_answers).sum().item()\n",
    "            total += pred_answers.numel()\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Test\n",
    "for name in train_dataset_names:\n",
    "    accuracy = evaluate_model(models[name], DataLoader(\n",
    "        test_datasets[name],\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    ))\n",
    "    print(f\"Testing model {name} accuracy on {name}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abf8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Jupyter venv)",
   "language": "python",
   "name": "jupyter-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
